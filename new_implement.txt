Advanced Computational Frameworks for High-Accuracy Pediatric Heart Murmur Detection in Multi-Location PhonocardiogramsThe detection of heart murmurs through automated phonocardiogram (PCG) analysis represents a critical frontier in computational cardiology, particularly in addressing the diagnostic limitations of traditional auscultation. While the identification of cardiac abnormalities might appear conceptually simple, the acoustic complexity of heart sounds—especially in what is categorized as "hard audio" characterized by high noise, low signal-to-noise ratios, and varying intensity—demands a multi-stage approach that integrates signal processing, temporal modeling, and deep learning.1 The efficacy of these models is typically evaluated on the George B. Moody PhysioNet Challenge 2022 dataset, also known as the CirCor DigiScope dataset, which remains the largest and most diverse collection of multi-location pediatric heart sounds.3Theoretical Foundations and the Mechanics of Cardiac AcousticsPhonocardiograms are discrete-time signals that record the mechanical vibrations of the heart. The primary cardiac sounds, $S_1$ and $S_2$, correspond to the closure of the atrioventricular and semilunar valves, respectively.2 Heart murmurs arise from turbulent blood flow, which may be indicative of underlying structural abnormalities such as valvular stenosis, regurgitation, or congenital heart disease.2 These murmurs are characterized by their location on the chest, timing within the cardiac cycle, duration, shape, intensity, and pitch.2The difficulty in training AI models to detect these sounds at high accuracy (above 95%) stems from the physiological and environmental variability inherent in the recordings. Environmental factors include ambient noise and microphone positioning, while physiological factors encompass breathing sounds, digestive noise, and variations in heart rate.6 Furthermore, murmurs often exhibit low intensity, making them difficult to distinguish from baseline noise in "hard audio" samples.ParameterClinical DescriptionData Representation in CirCor DigiScopeHeart Sound $S_1$Closure of Mitral/Tricuspid valvesInteger 1 in segmentation files 4SystolePeriod between $S_1$ and $S_2$Integer 2 in segmentation files 4Heart Sound $S_2$Closure of Aortic/Pulmonic valvesInteger 3 in segmentation files 4DiastolePeriod between $S_2$ and next $S_1$Integer 4 in segmentation files 4Murmur TimingEarly, Mid, Late, or HolosystolicCategorical label in subject metadata 4Auscultation PointsAV, PV, TV, MV, PhcMultichannel audio recordings per subject 2Analysis of Convolutional Neural Network Architectures and BaselinesThe current utilization of EfficientNet and ResNet architectures demonstrates the performance spectrum common in contemporary deep learning for audio. EfficientNet, while highly optimized for spatial feature extraction in images, often struggles with the specific temporal dependencies and spectral textures of PCG signals, resulting in reported accuracies around 65% when used without extensive domain-specific tuning.8 ResNet, through its use of residual connections, facilitates the training of deeper networks and mitigates the vanishing gradient problem, allowing it to capture more complex patterns in the spectrogram representation of the audio, thus achieving superior results around 82%.9The performance of these models is often benchmarked against the weighted accuracy metric used in the 2022 PhysioNet Challenge, which assesses the algorithm's ability to reproduce the findings of skilled human annotators across three classes: Present, Absent, and Unknown.11 The inclusion of the "Unknown" class is a significant hurdle, as these recordings often contain high levels of noise or ambiguous acoustic features that even experts cannot definitively classify.4ArchitectureObserved PerformancePrimary MechanismLimitations in PCG ContextEfficientNet~65% AccuracyCompound scaling of depth, width, and resolutionLack of native temporal bias for periodic signals 8ResNet~82% AccuracySkip connections and residual learningLimited global receptive field for long-range cycles 9Dual Bayesian ResNet0.82 (W.acc)Parallel binary classifications (Present/Others, Unknown/Others)High computational requirement for Bayesian inference 10Lightweight CNN0.747 (W.acc)Mel-spectrogram input with wide featuresSusceptibility to ambient noise and artifacts 13Reaching the 95% accuracy threshold necessitates a shift from these standard architectures toward specialized hybrid models, advanced signal processing, and the implementation of transformer-based systems.The McDonald et al. (CUED Acoustics) FrameworkThe research paper titled "Detection of Heart Murmurs in Phonocardiograms with Parallel Hidden Semi-Markov Models" by McDonald et al. presents a highly relevant and effective strategy for handling hard audio PCG signals.1 This approach was a top-performing entry in the 2022 PhysioNet Challenge, achieving a weighted accuracy of 0.776 on the hidden test set and winning the clinical outcomes task.1Architectural Mechanism: RNN-HSMM FusionThe core innovation of the McDonald et al. algorithm is the use of a Recurrent Neural Network (RNN) to provide observations for multiple parallel Hidden Semi-Markov Models (HSMMs). Unlike end-to-end models that attempt to classify the whole signal at once, this model first performs robust segmentation.1The signal is pre-processed into a log-spectrogram using a 50 ms Hann window with a 20 ms step, resulting in a 50 Hz feature sample rate.1 The frequency range is cropped to 0-800 Hz to eliminate higher-frequency noise that typically does not contain heart sound information.1 Each frequency row is then z-score normalized to reduce the dynamic range between murmurs and the loud $S_1$/$S_2$ sounds.1The predictive component uses a 3-layer bidirectional Gated Recurrent Unit (GRU) network with a hidden size of 60.1 This network outputs a five-state categorical probability for each time step: $S_1$, $S_2$, systole, diastole, and murmur.1 The inclusion of the "murmur" state directly into the segmentation RNN is a critical departure from previous work (like the Springer algorithm) that only modeled the four standard heart sound states.1Parallel HSMM and Viterbi DecodingTo finalize the classification, the model employs four parallel HSMMs, each making a different assumption about the signal's pathology:Normal Healthy Signal ($\omega_1$): A four-state model where the murmur posterior from the RNN is discarded.1Holosystolic Murmur ($\omega_2$): A four-state model where the murmur posterior replaces the standard systolic posterior.1Early-systolic Murmur ($\omega_3$): A five-state model requiring a transition from $S_1$ to murmur and then to systole.1Mid-systolic Murmur ($\omega_4$): A five-state model requiring a transition from $S_1$ to systole and then to murmur.1The final classification is determined by the model that achieves the highest segmentation confidence ($C_\omega$), calculated by tracing the Viterbi state path through the RNN posteriors as shown in the equation:$$\overline{C}_{\omega}=\frac{1}{T}\sum_{t=1}^{T}P(q_{t}=\hat{q}_{t}^{(\omega)}|x_{1:T},\theta_{R})$$Where $T$ is the number of windows and $q_t$ represents the state at time $t$.1 This approach is particularly effective for "hard audio" because the HSMM acts as a structural filter, rejecting RNN predictions that do not conform to the physiological timing of a heartbeat, thereby reducing false positives caused by transient noise.1Implementation Roadmap for High-Accuracy Murmur DetectionTo transition from 82% to 95% accuracy, several advanced techniques must be integrated into the training pipeline. These include the use of the Stockwell Transform, multi-task learning, and extensive data augmentation.Advanced Feature Extraction: The Stockwell TransformTraditional Mel-spectrograms may lose the phase information and fine-grained frequency resolution necessary to detect subtle murmurs in noisy audio. The Stockwell (S) transform has been proposed as a superior alternative.13 The S-transform is an extension of the continuous wavelet transform that maintains a frequency-dependent resolution while retaining the absolute phase of frequency components.13A model utilizing S-transform generated time-frequency maps (TFMs) paired with deep feature extraction via AlexNet has achieved an accuracy of 93% on the PhysioNet 2022 dataset.13 This methodology involves:Conversion: Transform the 1D PCG signal into a 2D TFM using the S-transform.13Feature Extraction: Use AlexNet (or a similar deep CNN) to extract high-level representations from the TFMs.13Dimensionality Reduction: Apply Recursive Feature Elimination (RFE) to remove redundant deep features.13Classification: Train a Random Forest (RF) classifier on the reduced feature set.13Transformer Models and Attention MechanismsThe user's inquiry regarding the effectiveness of transformer models is supported by current research. Transformers, particularly the Audio Spectrogram Transformer (AST), use self-attention mechanisms to model global relationships across the entire audio sequence.8 This is crucial for heart sounds, where the relationship between distant acoustic events (e.g., the interval between consecutive $S_1$ peaks) provides context for detecting murmurs.17Self-supervised learning (SSL) is also highly effective when labeled data is limited. The Masked Modeling Duo (M2D) model, a transformer-based SSL framework, has shown to outperform traditional CNNs by pre-training on large-scale datasets like AudioSet and fine-tuning on the CirCor DigiScope dataset, achieving a weighted accuracy of 0.832.19 The transformer's ability to handle variable-length input and provide a larger receptive field from the first layer makes it inherently more suitable for PCG analysis than standard CNNs.21Transformer ModelPre-training StrategyPerformance/ResultKey AdvantageAudio Spectrogram Transformer (AST)Supervised (AudioSet)95.6% on ESC-50Global context through self-attention 16Masked Modeling Duo (M2D)Self-supervised (AudioSet)0.832 W.acc (PCG)Effective representation from unlabeled data 19Whisper (Adapted)Large-scale AudioSOTA PerformanceRobustness to background noise 22BEATsZero-shot Audio70.1% AUROCRapid development and evaluation cycles 17Scattering TransformerTraining-free0.786 W.accComputationally efficient for edge devices 23Reaching 95% Plus Accuracy via Hybrid and MTL ModelsReaching the 95% accuracy mark often requires moving beyond single-task models. Multi-task learning (MTL) involves training the network to perform auxiliary tasks, such as heart rate estimation, alongside murmur detection.24 Apple Machine Learning Research demonstrated that a 2D-CNN-MTL model, utilizing four features (Mel-spectrogram, MFCCs, PSD, and RMSE), achieved murmur detection accuracy over 95% while maintaining precise heart rate estimates.24 The auxiliary task of heart rate estimation acts as a regularizer, forcing the model to learn the fundamental periodic structure of the heart sounds, which facilitates more accurate identification of murmurs.24Furthermore, extreme accuracy (up to 98.9%) has been reported when combining pre-trained Vision Transformers (ViT) with Multivariate MiniROCKET and aggressive data augmentation.26Strategies for Addressing "Hard Audio" and Data ImbalanceThe detection of murmurs in "hard audio" is significantly improved by data augmentation and signal synchronization. PCG signals from multiple auscultation locations should be treated as correlated channels rather than independent recordings.26Multi-Channel Synchronization and AugmentationA critical step in high-accuracy models is heartbeat synchronization. By identifying the peaks in recordings from all four valves and aligning them so that the first heartbeat of each file occurs at the same timestamp, the model can effectively leverage the varying intensity of a murmur across different locations.26Data augmentation is the most significant factor in pushing accuracy from the mid-80s to the high-90s.26 Research has shown that applying a combination of Gaussian noise injection, pitch shifting, time shifting, and time warping can increase accuracy from 76.0% (no augmentation) to 98.9% (heavy augmentation).7Augmentation TypeImplementation MechanismEffect on PCG DetectionGaussian NoiseElement-wise summation of random normal valuesImproves robustness to recording artifacts 7Pitch ShiftManipulating frequencies while preserving harmonicsSimulates physiological variations in murmur pitch 7Time ShiftRandom unidirectional shift of data signal indicesReduces sensitivity to the starting phase of the cycle 7Time WarpingTemporal manipulation via STFT independent of pitchSimulates variations in heart rate (tachycardia/bradycardia) 7SpecAugmentFrequency and time masking on spectrogramsPrevents overfitting to specific spectral features 20Handling Class ImbalanceThe CirCor DigiScope dataset is inherently imbalanced, with "Absent" cases making up approximately 73.8% of the data, "Present" cases 19.0%, and "Unknown" cases 7.2%.4 Standard accuracy metrics can be misleading in this context. To achieve clinical relevance, researchers use:Weighted Cross-Entropy Loss: Penalizing misclassifications of the minority "Present" and "Unknown" classes more heavily.1SMOTE (Synthetic Minority Over-sampling Technique): Generating synthetic examples for the "Present" and "Unknown" classes to balance the training distribution.27Stratified K-Fold Cross-Validation: Ensuring that each fold maintains the same ratio of murmur cases and that multiple recordings from the same patient are kept in the same fold to prevent data leakage.1Relevance and Implementation of the Parallel HSMM ModelThe McDonald et al. paper (CinC2022-020) is exceptionally relevant to the user's objective.1 While the user has explored ResNet (82% accuracy), the McDonald algorithm provides a blueprint for a system that is potentially more interpretable and robust to noise.1How the McDonald Model WorksThe "idea" behind the model is that end-to-end deep learning often ignores the biological structure of the heart sound. By using an RNN to predict the probabilities of different states and then using an HSMM to find the most likely sequence of those states that fits a "normal" or "murmur" heart cycle, the model essentially performs a logical check on its own predictions.1 If the RNN predicts a murmur during diastole when the clinical history suggests a systolic murmur, the HSMM will resolve this conflict based on the learned state durations.1Implementation DetailsTo implement the ideas from this paper, a developer should follow these steps derived from the study:Feature Pipeline: Calculate log-spectrograms with a 50 ms Hann window. Apply row-wise z-score normalization.1RNN Training: Construct a bidirectional GRU. The target labels should be the five categorical states ($S_1$, systole, $S_2$, diastole, murmur).1 Use cross-entropy loss weighted inversely to class frequency.1Duration Modeling: Estimate the heart rate using the autocorrelation of the RNN posteriors. Use this rate to scale normal distributions that represent the expected duration of each cardiac state.1Viterbi Path: Implement the duration-dependent Viterbi algorithm to find the optimal path through the states for each of the four parallel HSMM assumptions.1Decision Logic: If any of the three murmur-assuming models ($\omega_2, \omega_3, \omega_4$) has a higher confidence than the normal model ($\omega_1$), and the maximum confidence is above the signal quality threshold (0.65), classify as "Murmur Present".1Comparison of Hybrid and Pure Deep Learning ResultsThe search for 95% accuracy reveals a clear trend: purely architectural improvements (e.g., switching from ResNet to EfficientNet) are less effective than hybrid approaches that incorporate domain-specific signal processing or multi-task constraints.StudyApproachAccuracy/MetricClinical TaskMcDonald et al.RNN + Parallel HSMM0.776 W.acc (Test)Murmur Detection 1Manshadi & MihandoostS-Transform + AlexNet + RF93% AccuracyMurmur Detection 13Apple Research2dCNN-MTL>95% AccuracyMurmur Detection 24Care4MyHeartWavelet Ensemble Transformer0.855 (Train)Murmur Detection 27Valaee & ShiraniViT + MiniROCKET + Augmentation98.9% AccuracyMurmur Detection 26MK-RCNNMulti-Kernel Residual CNN98.33% AccuracyMurmur Detection 29FunnelNetCWT + SqueezeNet Hybrid99.70% AccuracyMurmur Detection 29These results indicate that while the user's current 82% accuracy is a strong baseline, reaching 95% is achievable through several verified pathways, most notably the use of multi-task learning or heavy data augmentation with vision transformers.Interpretability and Clinical Utility of Automated SystemsA critical aspect of high-accuracy models is their ability to provide interpretable results for clinicians. Segmentation-based models like the RNN-HSMM provide information about the location and timing of the murmur, which is more useful for symptomatic screening than a simple binary "present/absent" output.1 This is essential because murmurs heard at different chest locations have different levels of clinical significance.1Furthermore, the clinical outcome identification task (Task II of the PhysioNet Challenge) is significantly more difficult than murmur detection.3 While a murmur is an audible symptom, many cardiac abnormalities in the dataset do not produce an audible murmur.1 Algorithms must therefore learn to identify subtle acoustic signatures of abnormal cardiac function that may not be categorized as a classic murmur.1 The weighted accuracy metric for murmurs typically results in higher scores (top teams reaching ~0.78) compared to the cost-based metric for clinical outcomes, where even top models see performance decreases of up to 15% when moving from training to hidden test data.11Future Outlook and Technological ConvergenceThe field is moving toward a convergence of three distinct technologies:Foundational Audio Models: Using large-scale pre-trained transformers like Whisper or AST as feature extractors provides a level of robustness to noise that cannot be achieved by training from scratch on small datasets.22Structural Modeling: Incorporating physiological knowledge through models like HSMM or multi-task learning for heart rate ensures the AI's output remains biologically plausible.1Multi-Modal Fusion: Future systems are expected to combine PCG data with ECG, PPG, and patient biometrics (age, sex, height, weight) to provide a holistic assessment.1The use of patient demographic data is already common; models like the one from CUED Acoustics combine HSMM confidence scores with age and pregnancy status using CatBoost decision trees to predict clinical outcomes.1 This synthesis of acoustic and biometric data is likely the final step required to move automated cardiac auscultation from research into clinical practice.Synthesis of Findings for Enhanced Murmur DetectionFor a practitioner seeking to elevate their murmur detection accuracy from 82% to over 95%, the evidence suggests that the focus should shift from model depth to feature quality and data density. The Stockwell Transform and continuous wavelet transforms provide a multiresolution view of the "hard audio" that standard STFT-based spectrograms miss.13 Implementing the parallel HSMM architecture from the McDonald et al. paper allows for a structural filtering of noise, which is critical for maintaining high specificity in real-world recordings.1Ultimately, the most successful path to 95% accuracy on the current benchmark datasets involves the use of heavy data augmentation (pitch shifting, time warping, and noise injection) paired with a high-capacity model such as a vision transformer or a multi-task CNN-MTL architecture.24 These systems not only outperform standard ResNet baselines but also provide the sensitivity and robustness required for effective pediatric screening in resource-constrained environments.3Conclusions and Practical RecommendationsThe research indicates that achieving 95% plus accuracy in heart murmur detection is possible but requires moving beyond standard end-to-end CNN classification. The McDonald et al. paper provides a foundational "hybrid" idea that uses deep learning for local state prediction and traditional probabilistic models for global sequence structure.1 This approach addresses the user's need for a robust system capable of handling "hard audio" by incorporating physiological constraints.Furthermore, the implementation of transformer models, specifically the Audio Spectrogram Transformer or M2D, is highly recommended as an alternative to ResNet, provided they are fine-tuned on the CirCor DigiScope dataset after pre-training on larger audio repositories.19 For the highest possible accuracy, the integration of multi-channel synchronization—treating recordings from different valves as synchronized inputs—and the use of auxiliary tasks such as heart rate estimation represent the current state-of-the-art.24 These strategies, combined with aggressive data augmentation, fulfill the user's requirements for a high-accuracy detection system that is both effective and scientifically grounded.